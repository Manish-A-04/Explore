{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader,PyPDFLoader,Docx2txtLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = os.listdir(os.getcwd())\n",
    "file_paths.pop(0)\n",
    "file_paths.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_docs = []\n",
    "\n",
    "for path in file_paths:\n",
    "    \n",
    "    file_format = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    if file_format==\".pdf\":\n",
    "        print(\"Loading PDF\")\n",
    "        pdf_loader = PyPDFLoader(path)\n",
    "        combined_docs.extend(pdf_loader.load())\n",
    "    elif file_format==\".csv\":\n",
    "        print(\"Loading CSV \")\n",
    "        csv_loader = CSVLoader(path)\n",
    "        combined_docs.extend(csv_loader.load())\n",
    "    elif file_format==\".docx\":\n",
    "        print(\"Loading DOCX\")\n",
    "        docx_loader = Docx2txtLoader(path)\n",
    "        combined_docs.extend(docx_loader.load())\n",
    "    else:\n",
    "        print(\"File format is not supported\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(combined_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "db = Chroma.from_documents(splitted_docs , embedding_model, persist_directory=None)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import ollama\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19036\\2275093053.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = ollama.Ollama(model=\"mistral:latest\")\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\" , \"question\"],\n",
    "    template=\"You are an professional assistant,\\\n",
    "        based on what the user asks you should give a proper answer. \\\n",
    "            Make use of the context to generate an accurate answer. \\\n",
    "                <context>{context}</context> \\\n",
    "                    <question>{question}</question>\"\n",
    ")\n",
    "llm = ollama.Ollama(model=\"mistral:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,retriever=retriever,chain_type_kwargs={\"prompt\":prompt})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Language Learning Model (LLM) is a type of artificial intelligence model that is trained to generate human-like text. It learns patterns and structures in language by processing vast amounts of text data, such as books, websites, and conversation transcripts. The model uses techniques like transformers, autoregressive models, or encoder-decoders to predict the next word in a sequence based on the context provided. These models can be fine-tuned for various tasks, such as text generation, summarization, question answering, and more.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain.invoke(\"What is LLM\")\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Learning Model (LLM) is a type of artificial intelligence model that is trained to generate human-like text. It learns patterns and structures in language by processing vast amounts of text data, such as books, websites, and conversation transcripts. The model uses techniques like transformers, autoregressive models, or encoder-decoders to predict the next word in a sequence based on the context provided. These models can be fine-tuned for various tasks, such as text generation, summarization, question answering, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
