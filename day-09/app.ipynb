{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader , PyPDFLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File format is not supported\n",
      "Pdf is loading..........\n",
      "The PDF loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "curr_dir = os.getcwd()\n",
    "combined_docs = []\n",
    "for path in os.listdir(curr_dir):\n",
    "    file_fmt = os.path.splitext(path)[1]\n",
    "    if file_fmt==\".pdf\":\n",
    "        print(\"Pdf is loading..........\")\n",
    "        loader = PyPDFLoader(path)\n",
    "        combined_docs.extend(loader.load())\n",
    "        print(\"The PDF loaded successfully!\")\n",
    "    elif file_fmt==\".txt\":\n",
    "        print(\"The txt file is loading..........\")\n",
    "        loader = TextLoader(path)\n",
    "        combined_docs.extend(loader.load())\n",
    "        print(\"The TXT loaded successfully!\")\n",
    "    else:\n",
    "        print(\"File format is not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000 , chunk_overlap=200)\n",
    "splitted_docs = text_splitter.split_documents(combined_docs)\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "db = Chroma.from_documents(splitted_docs , embeddings_model)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import  SequentialChain \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "refining_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Given the question: {input},\n",
    "and the retrieved context: {context},\n",
    "refine the question for further retrieval.\n",
    "\"\"\", input_variables=[\"question\" , \"context\"]\n",
    ")\n",
    "final_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Use the refined question: {input} \n",
    "to retrieve more context and answer it. \n",
    "Context: {context}\n",
    "\"\"\",input_variables=[\"refined_question\" , \"context\"]\n",
    ")\n",
    "\n",
    "refining_1 = create_stuff_documents_chain(llm = llm , prompt=refining_prompt )\n",
    "refining_2 = create_retrieval_chain(retriever=retriever , combine_docs_chain=refining_1 )\n",
    "\n",
    "\n",
    "final_1 = create_stuff_documents_chain(llm=llm , prompt=refining_prompt)\n",
    "final_2 = create_retrieval_chain(retriever=retriever , combine_docs_chain=final_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question):\n",
    "    refined_question = refining_2.invoke({\"input\":question})[\"answer\"]\n",
    "    output = final_2.invoke({\"input\":refined_question})[\"answer\"]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, based on the original question and the retrieved context, let\\'s refine the question to target more specific aspects of DiffQRCoder.  Here are a few options, depending on what we want to learn more about:\\n\\n**Option 1 (Focus on SRPG):**\\n\\n*   **Refined Question:**  How does Scanning Robust Perceptual Guidance (SRPG) work within DiffQRCoder\\'s diffusion process to ensure scannability?  Specifically, what criteria or mechanisms are used within SRPG to guide the diffusion model toward generating scannable QR codes, and how does it maintain aesthetics?\\n\\n**Reasoning:** This option digs deeper into the core innovation of DiffQRCoder - the SRPG. It\\'s more specific than the original question and aims to understand the *how* behind its effectiveness.\\n\\n**Option 2 (Focus on the Two-Stage Process):**\\n\\n*   **Refined Question:**  The context mentions a \"two-stage iterative refinement framework.\" What are the two stages in DiffQRCoder\\'s process, and what is the role of SRPG and Scanning Robust Manifold Projected Gradient Descent (SR-MPGD) in each stage?  Why is a two-stage approach used?\\n\\n**Reasoning:** This explores the overall architecture of DiffQRCoder and the interplay between its key components. Understanding the two-stage nature provides a better grasp of the system as a whole.\\n\\n**Option 3 (Focus on SR-MPGD):**\\n\\n*   **Refined Question:** How does Scanning Robust Manifold Projected Gradient Descent (SR-MPGD) work as a post-processing technique in DiffQRCoder? How does it perform \"latent space optimization\" to enhance scanning robustness, and what are the key parameters (e.g., step size, LPIPS λ) that control its behavior?\\n\\n**Reasoning:** This focuses on the post-processing step, SR-MPGD.  It\\'s important to understand how this technique can bring the Scanning Success Rate (SSR) up to 100%.\\n\\n**Option 4 (Focus on Implementation Details):**\\n\\n*   **Refined Question:** What Diffusion Model and ControlNet architectures are used in DiffQRCoder? Are there any specific modifications or configurations applied to these architectures to optimize them for QR code generation?\\n\\n**Reasoning:** This delves into the more technical aspects of the implementation. It would be useful if you\\'re trying to replicate or build upon DiffQRCoder.\\n\\n**Which option is best depends on your goals.** If you want to understand the core innovation for scannability, choose Option 1. If you want to understand the overall system architecture, choose Option 2. If you\\'re interested in the post-processing step, choose Option 3. And if you want to know about the specific models used, choose Option 4.\\n\\nI recommend starting with Option 1 or 2, as they provide a more fundamental understanding of DiffQRCoder. Then, you can delve into the details of SR-MPGD (Option 3) or the implementation details (Option 4) if needed.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"How does qr based diffusion work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Okay, based on the original question and the retrieved context, let\\'s refine the question to target more specific aspects of DiffQRCoder.  Here are a few options, depending on what we want to learn more about:\\n\\n**Option 1 (Focus on SRPG):**\\n\\n*   **Refined Question:**  How does Scanning Robust Perceptual Guidance (SRPG) work within DiffQRCoder\\'s diffusion process to ensure scannability?  Specifically, what criteria or mechanisms are used within SRPG to guide the diffusion model toward generating scannable QR codes, and how does it maintain aesthetics?\\n\\n**Reasoning:** This option digs deeper into the core innovation of DiffQRCoder - the SRPG. It\\'s more specific than the original question and aims to understand the *how* behind its effectiveness.\\n\\n**Option 2 (Focus on the Two-Stage Process):**\\n\\n*   **Refined Question:**  The context mentions a \"two-stage iterative refinement framework.\" What are the two stages in DiffQRCoder\\'s process, and what is the role of SRPG and Scanning Robust Manifold Projected Gradient Descent (SR-MPGD) in each stage?  Why is a two-stage approach used?\\n\\n**Reasoning:** This explores the overall architecture of DiffQRCoder and the interplay between its key components. Understanding the two-stage nature provides a better grasp of the system as a whole.\\n\\n**Option 3 (Focus on SR-MPGD):**\\n\\n*   **Refined Question:** How does Scanning Robust Manifold Projected Gradient Descent (SR-MPGD) work as a post-processing technique in DiffQRCoder? How does it perform \"latent space optimization\" to enhance scanning robustness, and what are the key parameters (e.g., step size, LPIPS λ) that control its behavior?\\n\\n**Reasoning:** This focuses on the post-processing step, SR-MPGD.  It\\'s important to understand how this technique can bring the Scanning Success Rate (SSR) up to 100%.\\n\\n**Option 4 (Focus on Implementation Details):**\\n\\n*   **Refined Question:** What Diffusion Model and ControlNet architectures are used in DiffQRCoder? Are there any specific modifications or configurations applied to these architectures to optimize them for QR code generation?\\n\\n**Reasoning:** This delves into the more technical aspects of the implementation. It would be useful if you\\'re trying to replicate or build upon DiffQRCoder.\\n\\n**Which option is best depends on your goals.** If you want to understand the core innovation for scannability, choose Option 1. If you want to understand the overall system architecture, choose Option 2. If you\\'re interested in the post-processing step, choose Option 3. And if you want to know about the specific models used, choose Option 4.\\n\\nI recommend starting with Option 1 or 2, as they provide a more fundamental understanding of DiffQRCoder. Then, you can delve into the details of SR-MPGD (Option 3) or the implementation details (Option 4) if needed.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
