{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper as w\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\Desktop\\\\python practice\\\\Explore\\\\downloads\\\\How to implement Decision Trees from scratch with Python.mp3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "audio_path = \"\"\n",
    "for path in os.listdir(root_dir):\n",
    "    if path==\"downloads\":\n",
    "        audio = os.listdir(os.path.join(root_dir , path))\n",
    "        audio_path  = os.path.join(root_dir , path , audio[0])\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model  = w.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:01.720]  Hey, and welcome in this lesson,\n",
      "[00:01.720 --> 00:03.280]  we're going to learn how to implement\n",
      "[00:03.280 --> 00:05.040]  decision trees from scratch.\n",
      "[00:05.040 --> 00:07.960]  So let's first take a look at how decision trees work.\n",
      "[00:07.960 --> 00:09.720]  Let's say this is a data set that we have,\n",
      "[00:09.720 --> 00:13.440]  it's a data set of whether we can afford a house or not,\n",
      "[00:13.440 --> 00:16.680]  and it has information of which neighborhood it is in,\n",
      "[00:16.680 --> 00:17.800]  and how many rooms it has,\n",
      "[00:17.800 --> 00:19.960]  and also the affordability.\n",
      "[00:20.960 --> 00:24.480]  So when you want to build a decision tree, what you do\n",
      "[00:24.480 --> 00:26.840]  is let's say this is all of our data sets,\n",
      "[00:26.840 --> 00:29.440]  and this is basically our root note.\n",
      "[00:29.440 --> 00:33.280]  So we have one, two, three, four, five, six, seven, eight,\n",
      "[00:33.280 --> 00:36.600]  data points here representing all these data points here.\n",
      "[00:36.600 --> 00:38.440]  So what we say is at first, okay,\n",
      "[00:38.440 --> 00:42.000]  we want to divide which neighborhood this house is in.\n",
      "[00:42.000 --> 00:45.160]  So we can say if it's in the East neighborhood,\n",
      "[00:45.160 --> 00:46.920]  here's where all the data points are going,\n",
      "[00:46.920 --> 00:48.520]  if it's in the West neighborhood,\n",
      "[00:48.520 --> 00:50.640]  here's where all the data points are going.\n",
      "[00:50.640 --> 00:52.640]  And then you can do it again for the other column,\n",
      "[00:52.640 --> 00:54.880]  you can say, okay, how many number of rooms,\n",
      "[00:54.880 --> 00:56.640]  but you don't do it for each number,\n",
      "[00:56.640 --> 00:58.760]  you can say because this is an numerical value,\n",
      "[00:58.760 --> 01:02.040]  is it, for example, higher than five or not?\n",
      "[01:02.040 --> 01:05.680]  If it is higher than five, then it goes into one side,\n",
      "[01:05.680 --> 01:08.680]  if it's lower than five, it goes to the other side.\n",
      "[01:08.680 --> 01:13.200]  And now we have leaf notes, these are called leaf notes\n",
      "[01:14.200 --> 01:18.040]  that are really nice representing either yes or no.\n",
      "[01:18.040 --> 01:20.600]  So just to quickly recap, as I said,\n",
      "[01:20.600 --> 01:23.040]  this first note here is the root note\n",
      "[01:23.040 --> 01:25.080]  where all the data points are located.\n",
      "[01:25.080 --> 01:28.560]  And then if we go to a terminal note,\n",
      "[01:28.560 --> 01:31.440]  that is called a leaf note, so that's the end note.\n",
      "[01:31.440 --> 01:34.240]  And this left side or the right side,\n",
      "[01:34.240 --> 01:35.960]  after a note is called a branch,\n",
      "[01:35.960 --> 01:39.040]  so we're going to have left and right branches for our trees.\n",
      "[01:39.040 --> 01:40.320]  But in a tree like that, of course,\n",
      "[01:40.320 --> 01:42.120]  a couple of things need to be decided, right?\n",
      "[01:42.120 --> 01:46.200]  So for example, which feature do we split this branch with?\n",
      "[01:46.200 --> 01:47.800]  So right now we're in the root note,\n",
      "[01:47.800 --> 01:49.800]  how did we decide that we need to divide\n",
      "[01:49.800 --> 01:52.680]  on the neighborhood information or a neighborhood feature\n",
      "[01:52.680 --> 01:54.960]  and not on the room feature first?\n",
      "[01:54.960 --> 01:58.360]  Or when you are splitting in an numerical column\n",
      "[01:59.200 --> 02:00.760]  or feature which point should I split?\n",
      "[02:00.760 --> 02:03.200]  Should I split where the rooms are five,\n",
      "[02:03.200 --> 02:06.440]  either less or more than five or three or 10?\n",
      "[02:06.440 --> 02:08.000]  How do we decide that?\n",
      "[02:08.000 --> 02:10.800]  And lastly, when do we stop splitting?\n",
      "[02:10.800 --> 02:14.000]  So here we split two times maximum\n",
      "[02:14.000 --> 02:17.880]  and then we reach leaf notes where the leaf note\n",
      "[02:17.880 --> 02:22.280]  has a pure class, either yes or no, nothing mixed.\n",
      "[02:22.280 --> 02:24.520]  But sometimes you have a very big data set\n",
      "[02:24.520 --> 02:26.600]  and it is not possible to,\n",
      "[02:26.600 --> 02:28.960]  or you would have to build a very, very big tree\n",
      "[02:28.960 --> 02:31.960]  to be able to reach leaf notes that are pure.\n",
      "[02:31.960 --> 02:35.320]  So sometimes you have to decide when to stop beforehand.\n",
      "[02:35.320 --> 02:37.880]  So let's quickly go over how the trees are built\n",
      "[02:37.880 --> 02:39.080]  and how the trees are used\n",
      "[02:39.080 --> 02:41.800]  and then I'm going to point out some details from that.\n",
      "[02:41.800 --> 02:44.920]  So during training, what we do is given the whole data set,\n",
      "[02:44.920 --> 02:48.440]  we calculate the information gain from each possible split.\n",
      "[02:48.440 --> 02:50.240]  So this is each feature\n",
      "[02:50.240 --> 02:51.880]  and if the future is numerical,\n",
      "[02:51.880 --> 02:55.080]  each possible split inside this feature.\n",
      "[02:55.080 --> 02:58.600]  And then we divide this data set using this feature\n",
      "[02:58.600 --> 03:02.200]  and the value that gives us the most information gain.\n",
      "[03:02.200 --> 03:05.160]  And then we divide this node, so create two branches\n",
      "[03:05.160 --> 03:07.080]  and then we do this all over again\n",
      "[03:07.080 --> 03:10.080]  for all the newly created notes.\n",
      "[03:10.080 --> 03:13.120]  We do this until a stop in criteria is reached\n",
      "[03:13.120 --> 03:15.360]  as I mentioned before.\n",
      "[03:15.360 --> 03:17.400]  So here there are two things that you're seeing\n",
      "[03:17.400 --> 03:20.840]  that we don't know what they mean yet.\n",
      "[03:20.840 --> 03:22.200]  So one of them is information gain\n",
      "[03:22.200 --> 03:23.760]  and the other one is stop in criteria.\n",
      "[03:23.760 --> 03:25.360]  But we will see them in a second.\n",
      "[03:25.360 --> 03:27.720]  And during inference time, what we do is\n",
      "[03:27.720 --> 03:30.120]  we follow the tree based on the future values\n",
      "[03:30.120 --> 03:31.760]  that we have on our data point\n",
      "[03:31.760 --> 03:34.120]  until we reach a leaf node.\n",
      "[03:34.120 --> 03:36.520]  And if the leaf node is not pure,\n",
      "[03:36.520 --> 03:38.200]  if the leaf node is pure,\n",
      "[03:38.200 --> 03:40.560]  we return the class of the sleeve node.\n",
      "[03:40.560 --> 03:42.040]  And if the leaf node is not pure,\n",
      "[03:42.040 --> 03:43.200]  we do a majority vote.\n",
      "[03:43.200 --> 03:45.360]  So we return the most common class label.\n",
      "[03:45.360 --> 03:47.480]  All right, so we talked about a couple of terms.\n",
      "[03:47.480 --> 03:49.440]  One of them was information gain.\n",
      "[03:49.440 --> 03:52.920]  Information gain is calculated as the entropy\n",
      "[03:52.920 --> 03:55.400]  of the parent and the weighted average\n",
      "[03:55.400 --> 03:57.680]  of the entropy of the children.\n",
      "[03:57.680 --> 03:59.920]  So again, a new term entropy.\n",
      "[03:59.920 --> 04:03.160]  Entropy is basically the lack of order.\n",
      "[04:03.160 --> 04:06.320]  So if we look at our example from before, here,\n",
      "[04:06.320 --> 04:07.640]  in the state of the state,\n",
      "[04:07.640 --> 04:10.600]  the entropy of the root node is higher\n",
      "[04:10.600 --> 04:13.040]  than the entropy of the leaf nodes.\n",
      "[04:13.040 --> 04:14.640]  The entropy of the leaf node is zero\n",
      "[04:14.640 --> 04:16.040]  because there is no disorder.\n",
      "[04:16.040 --> 04:16.960]  It's all ordered.\n",
      "[04:16.960 --> 04:19.480]  There is only no values included.\n",
      "[04:19.480 --> 04:22.280]  Same with this leaf node and same with this leaf node.\n",
      "[04:22.280 --> 04:26.680]  Whereas for root, we have yes and no class labels included.\n",
      "[04:26.680 --> 04:28.640]  That's why the entropy is higher.\n",
      "[04:28.640 --> 04:31.600]  If the entropy, if the values there are 50-50,\n",
      "[04:31.600 --> 04:34.360]  we get the highest possible entropy, which is one.\n",
      "[04:34.360 --> 04:36.800]  So the entropy goes from zero to one.\n",
      "[04:36.800 --> 04:39.360]  So this is the equation that we use to calculate entropy.\n",
      "[04:39.360 --> 04:41.920]  And it's basically the sum of POFX\n",
      "[04:41.920 --> 04:45.120]  multiplied by log of POFX.\n",
      "[04:45.120 --> 04:46.520]  So what is POFX?\n",
      "[04:46.520 --> 04:49.840]  POFX is basically a number of times this class\n",
      "[04:49.840 --> 04:54.440]  has occurred in this node divided by the number of total nodes.\n",
      "[04:54.440 --> 04:56.640]  And lastly, stopping criteria.\n",
      "[04:56.640 --> 04:58.560]  So stopping criteria, as I mentioned,\n",
      "[04:58.560 --> 05:01.360]  sometimes a tree might grow a little bit too big.\n",
      "[05:01.360 --> 05:05.160]  You might not have time to go to all possible pure leaf nodes.\n",
      "[05:05.160 --> 05:08.000]  That's why you might want to stop your tree beforehand.\n",
      "[05:08.000 --> 05:10.960]  In that case, what you can use is maximum depth.\n",
      "[05:10.960 --> 05:14.760]  So how many layers of nodes are you going to allow your tree\n",
      "[05:14.760 --> 05:16.000]  to grow into?\n",
      "[05:16.000 --> 05:18.520]  And then you can use minimum number of samples, for example,\n",
      "[05:18.520 --> 05:22.360]  that is the minimum number of samples a node can have.\n",
      "[05:22.360 --> 05:25.600]  So if it has less than that amount of samples,\n",
      "[05:25.600 --> 05:27.880]  you are not going to divide that node anymore.\n",
      "[05:27.880 --> 05:30.720]  And lastly, you have minimum impurity decrease.\n",
      "[05:30.720 --> 05:32.720]  And that is the minimum entropy change\n",
      "[05:32.720 --> 05:36.120]  that needs to take place for a split to happen.\n",
      "[05:36.120 --> 05:39.280]  So let's start implementing decision trees now.\n",
      "[05:39.280 --> 05:41.320]  All right, let's get started with the implementation\n",
      "[05:41.320 --> 05:42.640]  of decision trees.\n",
      "[05:42.640 --> 05:44.760]  So there are two different classes I'm\n",
      "[05:44.760 --> 05:45.960]  going to create this time.\n",
      "[05:45.960 --> 05:47.280]  One of them is a node class.\n",
      "[05:47.280 --> 05:49.640]  And the other one is going to be a decision tree class,\n",
      "[05:49.640 --> 05:52.400]  because there are some things that I want to include\n",
      "[05:52.400 --> 05:54.160]  in the nodes too, if you remember.\n",
      "[05:54.160 --> 05:55.880]  This is one node, and this is one node,\n",
      "[05:55.880 --> 05:57.760]  and this whole thing is a tree.\n",
      "[05:57.760 --> 06:01.320]  And I want to keep the node and the tree separate.\n",
      "[06:01.320 --> 06:03.320]  So let's do that.\n",
      "[06:03.320 --> 06:04.800]  I have a node class.\n",
      "[06:09.560 --> 06:12.480]  And I also want to have a decision tree class.\n",
      "[06:17.600 --> 06:23.720]  All right, the things that I want to include in my node\n",
      "[06:23.720 --> 06:30.080]  is which feature this was divided with, which threshold,\n",
      "[06:30.080 --> 06:33.840]  this was divided with, the left tree,\n",
      "[06:33.840 --> 06:36.280]  the left node that we're pointing to,\n",
      "[06:36.280 --> 06:38.520]  the right tree that we're pointing to,\n",
      "[06:38.520 --> 06:41.040]  and also what the value of this tree is.\n",
      "[06:41.040 --> 06:46.840]  If this is a leaf node, otherwise, it's just going to be none.\n",
      "[06:47.360 --> 06:51.280]  So let's pass these by default.\n",
      "[06:51.280 --> 06:53.400]  This is going to be none.\n",
      "[06:53.400 --> 06:54.960]  Same with threshold.\n",
      "[06:57.400 --> 07:02.120]  And the left and right trees will also be none by default,\n",
      "[07:02.120 --> 07:03.920]  but we will pass them something.\n",
      "[07:03.920 --> 07:08.400]  And lastly, to pass the value, we can use this Python trick\n",
      "[07:08.400 --> 07:11.840]  that I saw from Python engineer of adding an asterisk\n",
      "[07:11.840 --> 07:15.680]  and then specifying this parameter.\n",
      "[07:15.720 --> 07:17.600]  So what happens is from now on,\n",
      "[07:17.600 --> 07:20.160]  if I want to pass the value parameter,\n",
      "[07:20.160 --> 07:21.520]  I have to pass it by name.\n",
      "[07:21.520 --> 07:23.680]  So I have to say, but I'm creating a node app,\n",
      "[07:23.680 --> 07:26.240]  to say node value equals this.\n",
      "[07:26.240 --> 07:27.800]  So it's going to be a bit more obvious\n",
      "[07:27.800 --> 07:30.160]  in our code when we're creating a leaf node\n",
      "[07:30.160 --> 07:33.080]  because the value will only be passed to leaf nodes.\n",
      "[07:33.080 --> 07:38.080]  So let me just write these threshold\n",
      "[07:40.160 --> 07:41.880]  and left will be left.\n",
      "[07:43.600 --> 07:45.600]  And right will be right.\n",
      "[07:45.640 --> 07:46.480]  All right.\n",
      "[07:46.480 --> 07:48.920]  And we can also include a little check here\n",
      "[07:48.920 --> 07:52.360]  to see if a specific node is a leaf node or not.\n",
      "[07:53.560 --> 07:58.400]  And then we'll say return self.\n",
      "[07:59.560 --> 08:01.080]  Here is not none.\n",
      "[08:01.080 --> 08:02.520]  And this will automatically tell me,\n",
      "[08:02.520 --> 08:04.480]  if the value exists, it means there's a leaf node.\n",
      "[08:04.480 --> 08:06.920]  So if I call this, it will tell me\n",
      "[08:06.920 --> 08:09.400]  if this node is a leaf node or not.\n",
      "[08:09.400 --> 08:11.400]  All right, so basically I'm done with the node\n",
      "[08:11.400 --> 08:13.400]  and now we can start with a decision tree.\n",
      "[08:13.400 --> 08:16.400]  So what we're going to have with the decision tree is again,\n",
      "[08:16.400 --> 08:19.120]  we're going to need a fit function.\n",
      "[08:19.120 --> 08:21.040]  We're going to need a predict function,\n",
      "[08:21.040 --> 08:25.000]  but we're going to have a bunch of helper functions, of course.\n",
      "[08:25.000 --> 08:26.200]  So let's start building.\n",
      "[08:26.200 --> 08:28.360]  Let's start with the initialization method.\n",
      "[08:28.360 --> 08:32.400]  We can start with some stuff in criteria.\n",
      "[08:32.400 --> 08:37.400]  So mill samples split, for example, or max depth.\n",
      "[08:38.120 --> 08:42.120]  And then we can have a number of features.\n",
      "[08:42.120 --> 08:45.120]  So number of features is something that we're going to get\n",
      "[08:45.120 --> 08:47.400]  from whoever is defining this tree,\n",
      "[08:47.400 --> 08:49.800]  while they're defining it at first.\n",
      "[08:49.800 --> 08:53.000]  It is a way to basically add some randomness to the tree.\n",
      "[08:53.000 --> 08:55.920]  So you're not using all of the features in your tree,\n",
      "[08:55.920 --> 08:57.560]  but just a subset of them, for example.\n",
      "[08:57.560 --> 08:59.440]  This is going to be specifically useful\n",
      "[08:59.440 --> 09:02.880]  when we're going to build random forests on top of the decision trees.\n",
      "[09:02.880 --> 09:05.400]  And of course, we want to keep the tree\n",
      "[09:05.920 --> 09:10.920]  and of course, we want to keep access to the root of the node\n",
      "[09:11.040 --> 09:13.240]  because that's when we're going to start traversing the tree\n",
      "[09:13.240 --> 09:15.320]  when during interference time.\n",
      "[09:15.320 --> 09:20.320]  So, min samples split.\n",
      "[09:20.760 --> 09:25.520]  Let's say for now, two max depth,\n",
      "[09:25.520 --> 09:30.520]  we can say 100 to begin with, but that can change.\n",
      "[09:30.640 --> 09:32.840]  And also a number of features.\n",
      "[09:36.360 --> 09:38.400]  We'll just say none for now.\n",
      "[09:39.520 --> 09:40.520]  All right.\n",
      "[09:46.400 --> 09:49.080]  Let's start with fitting our tree.\n",
      "[09:49.080 --> 09:52.520]  We need to pass it the X and Y values, of course.\n",
      "[09:52.520 --> 09:56.080]  And it's going to be a very big method\n",
      "[09:56.080 --> 09:57.880]  or very big function here.\n",
      "[09:57.880 --> 10:00.080]  So it's better to start dividing it up.\n",
      "[10:00.080 --> 10:02.400]  So I'll just create a\n",
      "[10:03.400 --> 10:05.880]  helper function called grow tree.\n",
      "[10:09.240 --> 10:13.600]  And that's going to return the root of the tree at the end.\n",
      "[10:13.600 --> 10:15.400]  And we're going to build the tree recursively,\n",
      "[10:15.400 --> 10:17.720]  but you'll see more about it in a second.\n",
      "[10:18.520 --> 10:24.720]  To this grow tree function, what we need to pass is the X and the Y values.\n",
      "[10:24.720 --> 10:28.360]  One thing that we need to check here is that the number of features\n",
      "[10:28.360 --> 10:32.320]  do not exceed the number of actual features that we have.\n",
      "[10:32.440 --> 10:39.760]  So we can just make sure that this number of features equals to X shape one.\n",
      "[10:40.040 --> 10:43.760]  So this shape gives us first a number of samples and a number of features.\n",
      "[10:43.760 --> 10:45.600]  You might remember from the previous lessons.\n",
      "[10:45.840 --> 10:49.440]  And we're saying it should be the number of features.\n",
      "[10:49.720 --> 10:50.720]  If\n",
      "[10:52.000 --> 10:57.400]  number of features from the initialization is not defined,\n",
      "[10:57.560 --> 11:02.400]  otherwise it needs to be the minimum in between this X shape\n",
      "[11:06.120 --> 11:07.720]  or the number of features.\n",
      "[11:09.880 --> 11:10.160]  All right.\n",
      "[11:10.160 --> 11:13.280]  So this is just to kind of make sure that we don't get an error here.\n",
      "[11:13.800 --> 11:20.120]  And from now on, we can actually start building our grow tree function.\n",
      "[11:20.720 --> 11:27.480]  As we said, it needs to get the self X and the Y.\n",
      "[11:28.720 --> 11:29.240]  All right.\n",
      "[11:30.200 --> 11:36.320]  So what we need to do here is basically first check the stopping criteria.\n",
      "[11:38.760 --> 11:40.960]  And then find,\n",
      "[11:45.000 --> 11:46.840]  find the best split.\n",
      "[11:47.840 --> 11:50.440]  Create child notes.\n",
      "[11:51.840 --> 11:55.800]  And basically call this grow tree function again.\n",
      "[11:55.800 --> 12:01.600]  So when we're creating a child notes, we're basically going to create the new trees,\n",
      "[12:01.600 --> 12:03.240]  sub trees in a way.\n",
      "[12:03.960 --> 12:06.720]  That's why this is going to be a recursive function.\n",
      "[12:07.320 --> 12:09.600]  So let's start with getting some information.\n",
      "[12:09.600 --> 12:13.720]  So as we said, we can get number of samples and number of features.\n",
      "[12:14.640 --> 12:15.560]  Let's call it feets.\n",
      "[12:16.560 --> 12:17.560]  From X shape.\n",
      "[12:18.640 --> 12:22.640]  And we would also like to have the number of labels.\n",
      "[12:22.640 --> 12:28.720]  And we can get that by using getting all the unique ones from Y.\n",
      "[12:28.720 --> 12:30.480]  But let me import numpy.\n",
      "[12:32.800 --> 12:33.520]  Here.\n",
      "[12:33.760 --> 12:34.280]  All right.\n",
      "[12:35.000 --> 12:37.480]  So we get all the unique values of Y.\n",
      "[12:37.480 --> 12:42.680]  And then we call it the number, the length will be the number of labels.\n",
      "[12:43.520 --> 12:44.800]  All right.\n",
      "[12:44.800 --> 12:50.840]  So some things that I want to check for are at first, I also need to actually pass a death\n",
      "[12:52.040 --> 12:53.560]  to grow tree.\n",
      "[12:53.560 --> 12:55.320]  So at first it's going to be zero.\n",
      "[12:55.320 --> 12:59.000]  And then as we're creating child notes, we're going to increase it by one.\n",
      "[12:59.640 --> 13:07.240]  So if the death depth is bigger or equal to self max depth,\n",
      "[13:08.200 --> 13:15.400]  we cannot go in there or if the number of labels that we just found is equals to one,\n",
      "[13:15.400 --> 13:18.200]  then we don't really want to split it again.\n",
      "[13:18.200 --> 13:20.200]  Of course, that means it's a leaf node.\n",
      "[13:20.200 --> 13:31.400]  And if number of samples in this current place is smaller than minimum sample split,\n",
      "[13:31.400 --> 13:35.480]  then we don't really want to grow the tree anymore.\n",
      "[13:35.640 --> 13:39.320]  Then what we're going to do is create a new node and then return it.\n",
      "[13:39.320 --> 13:45.400]  So then what I'm going to do is to return a leaf node and then pass it only a value\n",
      "[13:46.680 --> 13:47.880]  parameter.\n",
      "[13:47.880 --> 13:52.600]  But how we're going to do that is first, we need to calculate what the value is.\n",
      "[13:52.600 --> 13:56.600]  So if there are only one labels, of course, then it's easy what the value is.\n",
      "[13:56.600 --> 14:01.240]  But otherwise, we need to calculate or find out what the value is.\n",
      "[14:01.240 --> 14:04.040]  So for that, I'm going to create another help of function.\n",
      "[14:04.760 --> 14:07.240]  Most common label.\n",
      "[14:08.280 --> 14:11.480]  And I need to pass the Y to it, of course.\n",
      "[14:12.520 --> 14:13.960]  So let's define it here.\n",
      "[14:16.760 --> 14:23.800]  Most common label, but it's something similar in the previous lessons.\n",
      "[14:23.800 --> 14:25.720]  So you might remember it from that.\n",
      "[14:25.720 --> 14:32.760]  I need to first import from collections the counter data structure.\n",
      "[14:34.760 --> 14:37.880]  But not with the capital C here.\n",
      "[14:37.880 --> 14:38.280]  All right.\n",
      "[14:39.160 --> 14:44.440]  And then I just need to create one here with the Y values.\n",
      "[14:45.960 --> 14:51.480]  And on this counter, I just need to get the most common first one.\n",
      "[14:52.280 --> 14:55.160]  Of course, common one only.\n",
      "[14:55.160 --> 15:03.160]  And then I need to get the most common tuple and the first information that includes the value.\n",
      "[15:04.040 --> 15:06.760]  I showed you how this happened in the previous lesson.\n",
      "[15:06.760 --> 15:11.800]  I think probably in linear regression or no, in logistic regression.\n",
      "[15:11.800 --> 15:15.560]  So you can go check that one out or just go read documentation.\n",
      "[15:15.560 --> 15:16.520]  It's also there.\n",
      "[15:16.520 --> 15:17.880]  So this will be the value.\n",
      "[15:19.080 --> 15:23.240]  And then we will return value here.\n",
      "[15:24.200 --> 15:24.600]  Right.\n",
      "[15:25.320 --> 15:27.720]  So that way, we're getting the leaf value.\n",
      "[15:27.720 --> 15:29.560]  And then we can pass the leaf value here.\n",
      "[15:29.560 --> 15:35.000]  And then the tree, this part of the tree will be concluded basically.\n",
      "[15:35.640 --> 15:39.960]  But if this is not the case, if we do not get caught in the stopping criteria,\n",
      "[15:39.960 --> 15:42.680]  what we need to do is to find the best split.\n",
      "[15:42.680 --> 15:46.440]  And for that, why not just use another helper function?\n",
      "[15:46.440 --> 15:50.040]  So we'll create a helper function.\n",
      "[15:50.040 --> 15:52.120]  They'll tell me what the best split is.\n",
      "[15:52.120 --> 15:55.800]  I'm going to pass X to it, Y to it.\n",
      "[15:55.800 --> 16:01.560]  And the features that I want to create this, I want to include in this\n",
      "[16:02.680 --> 16:05.160]  consideration of creating a new split.\n",
      "[16:05.160 --> 16:10.840]  This is basically part of the place where we create the randomness in decision trees.\n",
      "[16:10.840 --> 16:16.360]  And it's going to return to me the best threshold and the best feature.\n",
      "[16:18.200 --> 16:25.720]  So I also need to pass it the feature IDs of course.\n",
      "[16:25.800 --> 16:28.520]  That's going to depend on self and features.\n",
      "[16:28.520 --> 16:31.880]  So how many features we want to consider at a time.\n",
      "[16:31.880 --> 16:35.160]  For that, I'm going to use numpy random choice.\n",
      "[16:35.160 --> 16:39.320]  We need to pass it a total number of features that we have.\n",
      "[16:39.960 --> 16:40.760]  Oh, that's feats.\n",
      "[16:42.120 --> 16:45.320]  And then number of features that we want to select\n",
      "[16:46.760 --> 16:48.520]  from our object.\n",
      "[16:48.520 --> 16:53.000]  And also, we're going to set replace to false by default.\n",
      "[16:53.000 --> 16:53.880]  It's true.\n",
      "[16:53.960 --> 16:57.480]  So this way, when we're selecting the random group,\n",
      "[16:57.480 --> 16:59.320]  we're not going to have duplicate ones.\n",
      "[16:59.320 --> 17:01.720]  So we're only going to have unique features.\n",
      "[17:02.520 --> 17:05.000]  Okay, so once this is done,\n",
      "[17:07.000 --> 17:11.400]  we can go ahead and create our helper function to select the best split.\n",
      "[17:16.920 --> 17:20.360]  Let's pass it what it needs, the X, Y,\n",
      "[17:21.080 --> 17:24.360]  and the feature in the Sizz.\n",
      "[17:30.360 --> 17:36.200]  Here, what we're going to do here is basically find the threshold among all possible thresholds\n",
      "[17:36.200 --> 17:38.600]  that are out there, all possible splits that are out there.\n",
      "[17:38.600 --> 17:40.440]  What's what the best one is?\n",
      "[17:40.440 --> 17:47.640]  So we can first say that we're going to keep the best gain value here and I'll set it to minus one\n",
      "[17:48.440 --> 17:55.160]  at first and then we'll also keep the split index and the split threshold.\n",
      "[17:55.160 --> 17:59.080]  These are the things that we're going to return to none at first.\n",
      "[18:00.680 --> 18:04.440]  And we're basically going to traverse all possible options here.\n",
      "[18:04.440 --> 18:12.520]  So for all features in feature indices,\n",
      "[18:13.080 --> 18:15.880]  we first need to get the column,\n",
      "[18:17.480 --> 18:23.000]  this column, and that's going to be X and the index.\n",
      "[18:23.000 --> 18:27.640]  So now I have only this column in my X column variable.\n",
      "[18:27.640 --> 18:31.000]  I need to get all the possible thresholds here.\n",
      "[18:31.000 --> 18:40.280]  So I can say MP, unique, again, give me all possible things that I can split with here.\n",
      "[18:40.360 --> 18:46.200]  And then I'm going to traverse again for all the thresholds in thresholds.\n",
      "[18:47.960 --> 18:50.680]  Maybe I'll call a THR to make it easier.\n",
      "[18:50.680 --> 18:59.560]  So here I need to calculate the information gain and once I calculate the information gain,\n",
      "[18:59.560 --> 19:05.560]  you can say gain, for example, we're going to calculate this with a helper function again.\n",
      "[19:05.640 --> 19:13.240]  And then I can say if gain is better than the best gain that we acquired so far,\n",
      "[19:15.480 --> 19:26.680]  the best gain is going to be gain and the split index is going to be this one,\n",
      "[19:26.680 --> 19:31.320]  the index that we're working on right now and the split threshold is going to be\n",
      "[19:31.880 --> 19:33.880]  the threshold.\n",
      "[19:37.800 --> 19:42.760]  And once we're done with this, we can return the split index and the threshold.\n",
      "[19:43.880 --> 19:46.120]  But I think I switched them here.\n",
      "[19:46.120 --> 19:46.840]  Yeah, okay.\n",
      "[19:47.640 --> 19:53.320]  So I'll just make sure that we're assigning it to the correct one.\n",
      "[19:53.320 --> 19:57.720]  So at first it's the feature and then the threshold and it is correct here.\n",
      "[19:58.680 --> 19:59.320]  All right.\n",
      "[19:59.960 --> 20:02.920]  Now what we need to do is to calculate the information gain.\n",
      "[20:02.920 --> 20:09.240]  So for that, I'm going to create another helper function here.\n",
      "[20:10.600 --> 20:12.040]  So let's define it.\n",
      "[20:14.120 --> 20:16.360]  Information gain.\n",
      "[20:17.080 --> 20:18.120]  It's going to be self.\n",
      "[20:19.080 --> 20:26.680]  What I need to pass it is the y and the x column and the threshold we're working with.\n",
      "[20:27.800 --> 20:34.520]  That was threshold THR.\n",
      "[20:34.520 --> 20:35.080]  All right.\n",
      "[20:39.960 --> 20:44.760]  Let's check out how we calculated how we're supposed to calculate the information gain.\n",
      "[20:44.760 --> 20:46.520]  So it is the entropy of a parent.\n",
      "[20:46.520 --> 20:49.960]  So it looks like we're going to need yet another helper function here.\n",
      "[20:49.960 --> 20:54.600]  And then the weighted average minus the weighted average of the entropy of the children.\n",
      "[20:54.600 --> 20:59.560]  So first I need to get the parent entropy.\n",
      "[21:00.440 --> 21:05.160]  Then I need to create children.\n",
      "[21:06.360 --> 21:09.080]  Then calculate the weighted\n",
      "[21:11.240 --> 21:13.720]  entropy of children.\n",
      "[21:13.720 --> 21:18.440]  And then finally calculate the information gain using all of these things.\n",
      "[21:18.440 --> 21:21.960]  But of course, we've weighted average of entropy of children.\n",
      "[21:22.600 --> 21:29.320]  So the parent entropy is going to be entropy.\n",
      "[21:30.360 --> 21:33.640]  Let's say self entropy.\n",
      "[21:34.600 --> 21:36.920]  And I'm going to pass it y.\n",
      "[21:38.600 --> 21:41.960]  Well, yeah, now that we're here, why don't we just define it actually?\n",
      "[21:43.560 --> 21:44.200]  Entropy.\n",
      "[21:45.960 --> 21:48.760]  It's going to get the path itself and y.\n",
      "[21:48.840 --> 21:52.040]  Let's see how we are supposed to calculate entropy.\n",
      "[21:52.680 --> 22:04.760]  So it is the summation of px times log 2 of px and px is, p of x is a number of times x has\n",
      "[22:04.760 --> 22:13.160]  occurred divided by the number of values in here. So I'm going to use a numpy trick to do this.\n",
      "[22:13.160 --> 22:15.560]  And I'll show you how I did that in a second.\n",
      "[22:15.640 --> 22:19.480]  First, I'm going to count the y's.\n",
      "[22:19.480 --> 22:25.080]  So what bing count does, kind of like a histogram, I'll show you over here.\n",
      "[22:25.080 --> 22:29.960]  So let's say I have an array y and the values are 1, 2, 3, 1, 2.\n",
      "[22:29.960 --> 22:36.600]  If I create a bing count for my y, what I'm going to get is 0, 2, 2, 1.\n",
      "[22:36.600 --> 22:40.200]  So what does that mean? It means 0 has occurred 0 times.\n",
      "[22:40.200 --> 22:44.440]  1 has occurred 2 times 2 has occurred 2 times and 3 has occurred 1 time.\n",
      "[22:44.440 --> 22:47.080]  Basically, I have like a nice little histogram here.\n",
      "[22:47.080 --> 22:54.040]  So the next thing that I'm going to do is to divide this histogram by the number of\n",
      "[22:55.400 --> 22:59.160]  values that I have. And why am I doing this?\n",
      "[22:59.160 --> 23:03.000]  This is basically the p of x if we see here.\n",
      "[23:03.000 --> 23:07.720]  It's basically a number of occurrences divided by the number of total values.\n",
      "[23:07.720 --> 23:12.440]  So that's what I'm doing here, but I'm doing it for all of them basically at once.\n",
      "[23:13.240 --> 23:22.040]  And the last thing we need to do for all the p's that we've created for p in p s.\n",
      "[23:23.000 --> 23:30.440]  If I'll just write in one line, if this p is bigger than 0, we calculate this part basically.\n",
      "[23:30.440 --> 23:39.960]  So p times log 2 of p. And nearly that's it.\n",
      "[23:40.040 --> 23:43.080]  Of course, I also need to sum these up.\n",
      "[23:44.680 --> 23:49.640]  And finally, I think we also need a minus sign here.\n",
      "[23:51.160 --> 23:55.160]  And return this and that will be our entropy.\n",
      "[23:56.440 --> 23:59.960]  All right, now with the this two, now we have the parent entropy.\n",
      "[23:59.960 --> 24:03.640]  Let's create the children and calculate their entropy's too.\n",
      "[24:03.640 --> 24:11.400]  So for that, I'm going to have to create yet another helper function here.\n",
      "[24:12.840 --> 24:17.800]  Call us just split for now. And split will get the column.\n",
      "[24:19.160 --> 24:21.320]  And that's where I show that we're getting here.\n",
      "[24:23.080 --> 24:23.880]  Let's create it.\n",
      "[24:24.040 --> 24:25.880]  Let's put it.\n",
      "[24:31.320 --> 24:35.480]  The x column and the splitting threshold.\n",
      "[24:36.840 --> 24:44.840]  And in here, I need to find which indices will go to the left and which will go to the right.\n",
      "[24:44.840 --> 24:49.160]  So for that, I'm going to use numpy argweir.\n",
      "[24:49.640 --> 24:54.120]  I'll show you how that one works.\n",
      "[24:54.120 --> 24:56.120]  So let's say this is my x column.\n",
      "[24:56.120 --> 24:58.120]  This is the values that I have in there.\n",
      "[24:58.120 --> 25:05.320]  If I say x argweir, let's say x smaller than 3.\n",
      "[25:05.320 --> 25:07.320]  Oops.\n",
      "[25:07.320 --> 25:11.320]  And numpy argweir.\n",
      "[25:11.320 --> 25:16.200]  Then it's going to tell me the 0, the first and the second one are smaller than 3.\n",
      "[25:16.680 --> 25:20.360]  But of course, I don't want it to have it in a list of lists way.\n",
      "[25:20.360 --> 25:22.040]  I want to have it in just one list.\n",
      "[25:22.040 --> 25:23.880]  So that's why I will say flatten.\n",
      "[25:23.880 --> 25:25.480]  So that's what we're going to do here.\n",
      "[25:26.680 --> 25:33.400]  I will say, give me all the ones where the value of the x column is smaller than the split threshold.\n",
      "[25:35.000 --> 25:37.400]  Smaller and equal to the split threshold.\n",
      "[25:38.360 --> 25:39.560]  And then flatten it.\n",
      "[25:39.560 --> 25:42.280]  So it's just an array.\n",
      "[25:43.640 --> 25:44.520]  Same here.\n",
      "[25:45.320 --> 25:47.800]  But except this time we're going to say bigger.\n",
      "[25:48.680 --> 25:49.800]  Let's separate these a bit.\n",
      "[25:49.800 --> 25:50.920]  So it's easier to read.\n",
      "[25:50.920 --> 25:57.080]  And then we will return left indices and the right indices.\n",
      "[25:58.120 --> 25:59.160]  And that's all actually.\n",
      "[25:59.160 --> 26:01.640]  Now we have the left and the right indices here.\n",
      "[26:01.640 --> 26:06.280]  After this, we should just check if one of these is empty.\n",
      "[26:06.280 --> 26:13.000]  So if the left indices is empty or the right indices is empty,\n",
      "[26:14.760 --> 26:21.080]  then we'll just return that the information gained from this split is zero.\n",
      "[26:21.080 --> 26:26.040]  Okay, now it's time to calculate the vaded entropy of the children.\n",
      "[26:26.760 --> 26:31.240]  So the first thing that I want to get is the length of the y.\n",
      "[26:32.200 --> 26:37.000]  How many samples we have in the y and in the left and in the right.\n",
      "[26:38.280 --> 26:41.400]  Left and number of samples in the right.\n",
      "[26:42.200 --> 26:43.000]  That is\n",
      "[26:45.080 --> 26:51.320]  length of the left indices and the length of the right indices.\n",
      "[26:51.320 --> 26:53.320]  And need to add the ss here.\n",
      "[26:56.920 --> 26:59.080]  And then we'll calculate their entropy.\n",
      "[27:00.040 --> 27:02.120]  Entropy of the left and entropy of the right.\n",
      "[27:02.120 --> 27:06.440]  We'll just going to pass it to the entropy helper function.\n",
      "[27:08.360 --> 27:09.560]  The left indices.\n",
      "[27:11.480 --> 27:12.840]  And the right indices.\n",
      "[27:14.440 --> 27:17.080]  But of course we need to get the actual values from it.\n",
      "[27:17.880 --> 27:20.760]  So not just the indices, but we need to find the\n",
      "[27:21.320 --> 27:24.040]  avi values that belong to these indices.\n",
      "[27:24.520 --> 27:28.760]  And finally, we calculate the calculated child entropy.\n",
      "[27:29.640 --> 27:31.720]  And let's see how we did that.\n",
      "[27:31.720 --> 27:35.400]  So it is basically the weighted average.\n",
      "[27:35.400 --> 27:39.640]  So weighted average in this case means how many samples are in one of them and how many\n",
      "[27:39.640 --> 27:41.720]  samples are in the other one.\n",
      "[27:41.720 --> 27:49.400]  So I need to find out a number of samples in the left one divided by a total number of samples\n",
      "[27:49.960 --> 27:57.560]  times left entropy plus number of samples in the right one divided by total number of samples\n",
      "[27:58.600 --> 28:00.920]  times the right entropy.\n",
      "[28:01.560 --> 28:03.640]  And that gives me the entropy of the children.\n",
      "[28:03.640 --> 28:05.400]  We have the entropy of the parent.\n",
      "[28:05.480 --> 28:09.960]  So the only thing that we need to do is to find the information gain\n",
      "[28:11.080 --> 28:13.880]  and the information, not grain, gain.\n",
      "[28:14.600 --> 28:21.160]  And that is parent entropy minus child entropy.\n",
      "[28:22.120 --> 28:23.880]  And then we return information gain.\n",
      "[28:25.640 --> 28:26.280]  There you go.\n",
      "[28:26.280 --> 28:28.280]  We also finished the information gain function.\n",
      "[28:28.280 --> 28:30.520]  So let's go back where did we leave it?\n",
      "[28:31.560 --> 28:31.960]  All right.\n",
      "[28:31.960 --> 28:34.600]  So what happens is when we call fit\n",
      "[28:35.960 --> 28:42.040]  we check that the number of features is not more than the actual number of features.\n",
      "[28:42.040 --> 28:47.960]  We call grow tree, grow tree checks, the best feature here.\n",
      "[28:47.960 --> 28:53.960]  I first check sort of something criteria and then calls best split to find the best split.\n",
      "[28:53.960 --> 28:57.720]  And the best split calls information gain.\n",
      "[28:57.720 --> 29:01.560]  Information gain calls entropy and split.\n",
      "[29:01.560 --> 29:02.840]  And split is run.\n",
      "[29:02.920 --> 29:10.360]  It tells it what the information, what the left and the right trees are.\n",
      "[29:11.400 --> 29:14.280]  I'm just checking that I don't have any typos at the same time.\n",
      "[29:15.480 --> 29:17.640]  We calculate the entropy of the children.\n",
      "[29:19.320 --> 29:19.720]  Like this.\n",
      "[29:22.440 --> 29:23.320]  Let's see.\n",
      "[29:23.320 --> 29:25.240]  And the information gain is calculated.\n",
      "[29:25.240 --> 29:27.240]  It's passed back to best split.\n",
      "[29:27.240 --> 29:28.840]  Best split is calculated here.\n",
      "[29:28.840 --> 29:31.000]  And then it goes back to grow the tree.\n",
      "[29:31.640 --> 29:32.200]  All right.\n",
      "[29:32.200 --> 29:36.360]  So the last thing that we need to do is to create the child nodes then.\n",
      "[29:36.360 --> 29:38.280]  And we already did part of it actually.\n",
      "[29:38.280 --> 29:44.040]  So we call left indices and right indices.\n",
      "[29:45.160 --> 29:47.000]  And guess what we're going to call for that?\n",
      "[29:47.000 --> 29:50.040]  We're going to call split because split is a thing that calculates\n",
      "[29:50.680 --> 29:52.840]  what the left and right indices should be.\n",
      "[29:52.840 --> 29:57.400]  And the split requires x column and the split threshold.\n",
      "[29:57.400 --> 29:59.320]  We know what the split threshold is.\n",
      "[29:59.320 --> 30:00.680]  It is the best threshold.\n",
      "[30:02.440 --> 30:09.080]  And the x column in this case is the column that belongs to the best\n",
      "[30:09.800 --> 30:10.280]  threshold.\n",
      "[30:10.280 --> 30:11.560]  So the best feature here.\n",
      "[30:12.920 --> 30:14.920]  This will give me left and right indices.\n",
      "[30:14.920 --> 30:17.080]  And now I have to call the grow tree\n",
      "[30:17.960 --> 30:20.200]  using these left and right indices values.\n",
      "[30:20.920 --> 30:24.920]  So the left would be grow tree.\n",
      "[30:25.720 --> 30:28.520]  Makes left indices.\n",
      "[30:34.120 --> 30:34.920]  What else do we need?\n",
      "[30:34.920 --> 30:35.720]  We need the y.\n",
      "[30:36.760 --> 30:38.600]  Y left indices.\n",
      "[30:41.800 --> 30:44.920]  And finally, we need to increase the depth by one.\n",
      "[30:46.200 --> 30:49.560]  The right will be more or less the same self.\n",
      "[30:49.560 --> 30:50.280]  Grow tree.\n",
      "[30:51.800 --> 30:53.080]  The right indices.\n",
      "[30:55.880 --> 30:58.280]  And y right indices.\n",
      "[31:00.680 --> 31:02.520]  And again, the depth will be plus one.\n",
      "[31:04.040 --> 31:07.400]  And what we want to return here is a new node that we created\n",
      "[31:07.960 --> 31:13.880]  with the information of what we divided it with, the best feature we divided it with,\n",
      "[31:15.160 --> 31:17.560]  the threshold that we've divided it with.\n",
      "[31:18.600 --> 31:19.560]  What else do we have here?\n",
      "[31:19.560 --> 31:20.520]  And left and right.\n",
      "[31:21.240 --> 31:23.240]  You also need to pass it left.\n",
      "[31:25.880 --> 31:27.000]  And right.\n",
      "[31:27.960 --> 31:31.720]  All right, so it looks like the part where we do the training actually ended.\n",
      "[31:31.720 --> 31:35.160]  The next thing that we want to do is to write the predict function.\n",
      "[31:35.160 --> 31:36.200]  Let's find that here.\n",
      "[31:37.400 --> 31:37.800]  Right.\n",
      "[31:38.600 --> 31:39.960]  So let's pass it to self.\n",
      "[31:41.400 --> 31:43.960]  And an x test that we're going to get.\n",
      "[31:43.960 --> 31:46.440]  So what I want to do is for every x\n",
      "[31:47.560 --> 31:49.800]  and the set that is being passed to me,\n",
      "[31:50.360 --> 31:54.600]  I want to traverse the tree to find the result.\n",
      "[31:55.800 --> 31:59.480]  So one last helper function here.\n",
      "[31:59.480 --> 32:03.320]  And that will return to me a list of results.\n",
      "[32:03.960 --> 32:08.520]  And then I will parse it into a non-py array and return it.\n",
      "[32:11.000 --> 32:15.720]  All right, let's start our last helper function, traverse tree.\n",
      "[32:15.720 --> 32:18.760]  So the traversing of the tree will also happen\n",
      "[32:21.400 --> 32:22.840]  in a recursive way.\n",
      "[32:22.920 --> 32:24.280]  So I need to pass it to x.\n",
      "[32:24.280 --> 32:27.080]  But I also need to pass it a node that we're starting from.\n",
      "[32:27.080 --> 32:30.280]  So it first is going to be self root.\n",
      "[32:30.280 --> 32:31.720]  We're going to start from the root.\n",
      "[32:31.720 --> 32:36.280]  First, I want to check if this node is leaf node.\n",
      "[32:37.080 --> 32:39.480]  What was the function there is leaf node.\n",
      "[32:39.480 --> 32:44.360]  If that is the case, then we return the value of the node.\n",
      "[32:45.480 --> 32:50.680]  So if you remember, when we're creating the left and right nodes here,\n",
      "[32:51.480 --> 32:54.760]  we are passing them a certain feature,\n",
      "[32:55.560 --> 32:58.360]  which feature that we divided it with.\n",
      "[32:58.360 --> 33:05.160]  So we just need to find the feature that it's divided with.\n",
      "[33:05.880 --> 33:10.520]  If the value is smaller or equal to the threshold,\n",
      "[33:13.400 --> 33:17.800]  then we pass the left side of the tree to be traversed next.\n",
      "[33:17.800 --> 33:21.400]  Or we pass the right side of the tree to be traversed next.\n",
      "[33:21.400 --> 33:25.400]  So we return, call traverse tree again.\n",
      "[33:26.200 --> 33:30.920]  We pass it to x again and now the starting node will be node left.\n",
      "[33:32.600 --> 33:35.160]  Otherwise, though, if that is not the case,\n",
      "[33:36.920 --> 33:39.720]  then we pass it the right.\n",
      "[33:39.720 --> 33:46.600]  And yeah, that's all that we need to do to traverse the tree.\n",
      "[33:46.600 --> 33:48.280]  We check if it's a leaf node.\n",
      "[33:48.280 --> 33:50.280]  If that's the case, we pass the value.\n",
      "[33:51.080 --> 33:53.480]  So basically, everything will end up here.\n",
      "[33:53.480 --> 33:57.720]  All of the paths that we follow are going to end up in the leaf node anyways.\n",
      "[33:57.720 --> 34:02.040]  But then we're going to get values and we're going to get one value for each\n",
      "[34:02.680 --> 34:05.240]  number in our test set.\n",
      "[34:05.240 --> 34:09.720]  And then we're going to turn it into an NMPI array and we're going to return it.\n",
      "[34:09.720 --> 34:12.520]  Awesome. So it looks like we did everything.\n",
      "[34:12.600 --> 34:17.880]  Let's run it and see if we made any mistakes or what the accuracy is going to be like.\n",
      "[34:17.880 --> 34:22.120]  So for that, I'm importing the breast cancer data set again from CycatLearn.\n",
      "[34:23.560 --> 34:28.760]  Need to import the decision tree and then we'll create a decision tree.\n",
      "[34:29.720 --> 34:34.440]  And the first run, I'll just keep the default values that we already decided on.\n",
      "[34:34.440 --> 34:38.680]  And then we're going to fit the decision tree with X train and Y train.\n",
      "[34:38.680 --> 34:45.640]  And then we're going to get predictions using the predict method.\n",
      "[34:45.640 --> 34:49.560]  And I'm going to pass it the X test for that.\n",
      "[34:49.560 --> 34:56.120]  To see the results and how good the results are, of course, we need to have a metric.\n",
      "[34:56.120 --> 34:58.760]  We'll call accuracy for now.\n",
      "[34:58.760 --> 35:03.320]  I'll call predictions or maybe let's call Y-Pred.\n",
      "[35:03.320 --> 35:06.200]  Y-Tests and Y-Pred.\n",
      "[35:06.680 --> 35:15.240]  And that's basically the number of times Y-Tests equals to Y-Pred divided by\n",
      "[35:16.280 --> 35:21.160]  length of the Y-Test. And we can just return this.\n",
      "[35:22.840 --> 35:26.760]  And yeah, let's calculate the accuracy.\n",
      "[35:26.760 --> 35:30.200]  I'll call it ACC.\n",
      "[35:30.200 --> 35:36.360]  And it's going to be again, predictions and Y-Test.\n",
      "[35:36.360 --> 35:40.360]  And let's print it afterwards.\n",
      "[35:40.360 --> 35:43.400]  All right, I'm excited. Let's see.\n",
      "[35:43.400 --> 35:47.400]  Oh, there.\n",
      "[35:47.400 --> 35:49.400]  Line 51.\n",
      "[35:49.400 --> 35:52.040]  Did a little comma here.\n",
      "[35:52.680 --> 35:54.600]  Accidentally. Let's try again.\n",
      "[35:57.240 --> 36:00.360]  All right, it took me a second, but I found what I did wrong here.\n",
      "[36:00.360 --> 36:03.640]  And the definition, the initialization of the node,\n",
      "[36:03.640 --> 36:08.280]  instead of self-value, equals to none, it should have been value and not none.\n",
      "[36:08.280 --> 36:11.400]  And while I was figuring it out, I found another problem.\n",
      "[36:11.400 --> 36:15.960]  In the traverse tree function, I was returning nodes value.\n",
      "[36:15.960 --> 36:17.240]  But then I was calling the value.\n",
      "[36:17.240 --> 36:18.280]  I'm not supposed to call it.\n",
      "[36:18.280 --> 36:21.880]  I'm just supposed to return the value of this node.\n",
      "[36:21.960 --> 36:25.000]  All right, so let's run this once again and see what we get.\n",
      "[36:25.000 --> 36:30.840]  All right, so we get a accuracy of 0.91, which is pretty good,\n",
      "[36:31.720 --> 36:34.840]  for something that we wrote in a couple of minutes.\n",
      "[36:34.840 --> 36:39.640]  And let's try passing it some other arguments, for example,\n",
      "[36:39.640 --> 36:40.600]  mac step 10.\n",
      "[36:42.520 --> 36:46.680]  Okay, we get a bit of a better accuracy here, but yeah,\n",
      "[36:46.680 --> 36:50.520]  you can play around with this and see what you get better on this.\n",
      "[36:51.240 --> 36:52.840]  Brescancere data sets.\n",
      "[36:52.840 --> 36:54.120]  I hope everything was clear.\n",
      "[36:54.120 --> 36:56.120]  I know this was a bit of a longer one,\n",
      "[36:56.120 --> 36:59.480]  because there's a bit more involved to create decision trees.\n",
      "[36:59.480 --> 37:02.040]  If you have any questions, don't forget to leave them as a comment.\n",
      "[37:02.040 --> 37:03.720]  We'll be happy to help you there.\n",
      "[37:03.720 --> 37:07.000]  And if you want this code, you can find it in our GitHub repository,\n",
      "[37:07.000 --> 37:08.840]  as always, the link is in the description.\n",
      "[37:08.840 --> 37:11.560]  But I'm looking forward to seeing you in the next lesson.\n"
     ]
    }
   ],
   "source": [
    "transcription = audio_model.transcribe(audio_path , verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hey, and welcome in this lesson, we're going to learn how to implement decision trees from scratch. So let's first take a look at how decision trees work. Let's say this is a data set that we have, it's a data set of whether we can afford a house or not, and it has information of which neighborhood it is in, and how many rooms it has, and also the affordability. So when you want to build a decision tree, what you do is let's say this is all of our data sets, and this is basically our root note. So we have one, two, three, four, five, six, seven, eight, data points here representing all these data points here. So what we say is at first, okay, we want to divide which neighborhood this house is in. So we can say if it's in the East neighborhood, here's where all the data points are going, if it's in the West neighborhood, here's where all the data points are going. And then you can do it again for the other column, you can say, okay, how many number of rooms, but you don't do it for each number, you can say because this is an numerical value, is it, for example, higher than five or not? If it is higher than five, then it goes into one side, if it's lower than five, it goes to the other side. And now we have leaf notes, these are called leaf notes that are really nice representing either yes or no. So just to quickly recap, as I said, this first note here is the root note where all the data points are located. And then if we go to a terminal note, that is called a leaf note, so that's the end note. And this left side or the right side, after a note is called a branch, so we're going to have left and right branches for our trees. But in a tree like that, of course, a couple of things need to be decided, right? So for example, which feature do we split this branch with? So right now we're in the root note, how did we decide that we need to divide on the neighborhood information or a neighborhood feature and not on the room feature first? Or when you are splitting in an numerical column or feature which point should I split? Should I split where the rooms are five, either less or more than five or three or 10? How do we decide that? And lastly, when do we stop splitting? So here we split two times maximum and then we reach leaf notes where the leaf note has a pure class, either yes or no, nothing mixed. But sometimes you have a very big data set and it is not possible to, or you would have to build a very, very big tree to be able to reach leaf notes that are pure. So sometimes you have to decide when to stop beforehand. So let's quickly go over how the trees are built and how the trees are used and then I'm going to point out some details from that. So during training, what we do is given the whole data set, we calculate the information gain from each possible split. So this is each feature and if the future is numerical, each possible split inside this feature. And then we divide this data set using this feature and the value that gives us the most information gain. And then we divide this node, so create two branches and then we do this all over again for all the newly created notes. We do this until a stop in criteria is reached as I mentioned before. So here there are two things that you're seeing that we don't know what they mean yet. So one of them is information gain and the other one is stop in criteria. But we will see them in a second. And during inference time, what we do is we follow the tree based on the future values that we have on our data point until we reach a leaf node. And if the leaf node is not pure, if the leaf node is pure, we return the class of the sleeve node. And if the leaf node is not pure, we do a majority vote. So we return the most common class label. All right, so we talked about a couple of terms. One of them was information gain. Information gain is calculated as the entropy of the parent and the weighted average of the entropy of the children. So again, a new term entropy. Entropy is basically the lack of order. So if we look at our example from before, here, in the state of the state, the entropy of the root node is higher than the entropy of the leaf nodes. The entropy of the leaf node is zero because there is no disorder. It's all ordered. There is only no values included. Same with this leaf node and same with this leaf node. Whereas for root, we have yes and no class labels included. That's why the entropy is higher. If the entropy, if the values there are 50-50, we get the highest possible entropy, which is one. So the entropy goes from zero to one. So this is the equation that we use to calculate entropy. And it's basically the sum of POFX multiplied by log of POFX. So what is POFX? POFX is basically a number of times this class has occurred in this node divided by the number of total nodes. And lastly, stopping criteria. So stopping criteria, as I mentioned, sometimes a tree might grow a little bit too big. You might not have time to go to all possible pure leaf nodes. That's why you might want to stop your tree beforehand. In that case, what you can use is maximum depth. So how many layers of nodes are you going to allow your tree to grow into? And then you can use minimum number of samples, for example, that is the minimum number of samples a node can have. So if it has less than that amount of samples, you are not going to divide that node anymore. And lastly, you have minimum impurity decrease. And that is the minimum entropy change that needs to take place for a split to happen. So let's start implementing decision trees now. All right, let's get started with the implementation of decision trees. So there are two different classes I'm going to create this time. One of them is a node class. And the other one is going to be a decision tree class, because there are some things that I want to include in the nodes too, if you remember. This is one node, and this is one node, and this whole thing is a tree. And I want to keep the node and the tree separate. So let's do that. I have a node class. And I also want to have a decision tree class. All right, the things that I want to include in my node is which feature this was divided with, which threshold, this was divided with, the left tree, the left node that we're pointing to, the right tree that we're pointing to, and also what the value of this tree is. If this is a leaf node, otherwise, it's just going to be none. So let's pass these by default. This is going to be none. Same with threshold. And the left and right trees will also be none by default, but we will pass them something. And lastly, to pass the value, we can use this Python trick that I saw from Python engineer of adding an asterisk and then specifying this parameter. So what happens is from now on, if I want to pass the value parameter, I have to pass it by name. So I have to say, but I'm creating a node app, to say node value equals this. So it's going to be a bit more obvious in our code when we're creating a leaf node because the value will only be passed to leaf nodes. So let me just write these threshold and left will be left. And right will be right. All right. And we can also include a little check here to see if a specific node is a leaf node or not. And then we'll say return self. Here is not none. And this will automatically tell me, if the value exists, it means there's a leaf node. So if I call this, it will tell me if this node is a leaf node or not. All right, so basically I'm done with the node and now we can start with a decision tree. So what we're going to have with the decision tree is again, we're going to need a fit function. We're going to need a predict function, but we're going to have a bunch of helper functions, of course. So let's start building. Let's start with the initialization method. We can start with some stuff in criteria. So mill samples split, for example, or max depth. And then we can have a number of features. So number of features is something that we're going to get from whoever is defining this tree, while they're defining it at first. It is a way to basically add some randomness to the tree. So you're not using all of the features in your tree, but just a subset of them, for example. This is going to be specifically useful when we're going to build random forests on top of the decision trees. And of course, we want to keep the tree and of course, we want to keep access to the root of the node because that's when we're going to start traversing the tree when during interference time. So, min samples split. Let's say for now, two max depth, we can say 100 to begin with, but that can change. And also a number of features. We'll just say none for now. All right. Let's start with fitting our tree. We need to pass it the X and Y values, of course. And it's going to be a very big method or very big function here. So it's better to start dividing it up. So I'll just create a helper function called grow tree. And that's going to return the root of the tree at the end. And we're going to build the tree recursively, but you'll see more about it in a second. To this grow tree function, what we need to pass is the X and the Y values. One thing that we need to check here is that the number of features do not exceed the number of actual features that we have. So we can just make sure that this number of features equals to X shape one. So this shape gives us first a number of samples and a number of features. You might remember from the previous lessons. And we're saying it should be the number of features. If number of features from the initialization is not defined, otherwise it needs to be the minimum in between this X shape or the number of features. All right. So this is just to kind of make sure that we don't get an error here. And from now on, we can actually start building our grow tree function. As we said, it needs to get the self X and the Y. All right. So what we need to do here is basically first check the stopping criteria. And then find, find the best split. Create child notes. And basically call this grow tree function again. So when we're creating a child notes, we're basically going to create the new trees, sub trees in a way. That's why this is going to be a recursive function. So let's start with getting some information. So as we said, we can get number of samples and number of features. Let's call it feets. From X shape. And we would also like to have the number of labels. And we can get that by using getting all the unique ones from Y. But let me import numpy. Here. All right. So we get all the unique values of Y. And then we call it the number, the length will be the number of labels. All right. So some things that I want to check for are at first, I also need to actually pass a death to grow tree. So at first it's going to be zero. And then as we're creating child notes, we're going to increase it by one. So if the death depth is bigger or equal to self max depth, we cannot go in there or if the number of labels that we just found is equals to one, then we don't really want to split it again. Of course, that means it's a leaf node. And if number of samples in this current place is smaller than minimum sample split, then we don't really want to grow the tree anymore. Then what we're going to do is create a new node and then return it. So then what I'm going to do is to return a leaf node and then pass it only a value parameter. But how we're going to do that is first, we need to calculate what the value is. So if there are only one labels, of course, then it's easy what the value is. But otherwise, we need to calculate or find out what the value is. So for that, I'm going to create another help of function. Most common label. And I need to pass the Y to it, of course. So let's define it here. Most common label, but it's something similar in the previous lessons. So you might remember it from that. I need to first import from collections the counter data structure. But not with the capital C here. All right. And then I just need to create one here with the Y values. And on this counter, I just need to get the most common first one. Of course, common one only. And then I need to get the most common tuple and the first information that includes the value. I showed you how this happened in the previous lesson. I think probably in linear regression or no, in logistic regression. So you can go check that one out or just go read documentation. It's also there. So this will be the value. And then we will return value here. Right. So that way, we're getting the leaf value. And then we can pass the leaf value here. And then the tree, this part of the tree will be concluded basically. But if this is not the case, if we do not get caught in the stopping criteria, what we need to do is to find the best split. And for that, why not just use another helper function? So we'll create a helper function. They'll tell me what the best split is. I'm going to pass X to it, Y to it. And the features that I want to create this, I want to include in this consideration of creating a new split. This is basically part of the place where we create the randomness in decision trees. And it's going to return to me the best threshold and the best feature. So I also need to pass it the feature IDs of course. That's going to depend on self and features. So how many features we want to consider at a time. For that, I'm going to use numpy random choice. We need to pass it a total number of features that we have. Oh, that's feats. And then number of features that we want to select from our object. And also, we're going to set replace to false by default. It's true. So this way, when we're selecting the random group, we're not going to have duplicate ones. So we're only going to have unique features. Okay, so once this is done, we can go ahead and create our helper function to select the best split. Let's pass it what it needs, the X, Y, and the feature in the Sizz. Here, what we're going to do here is basically find the threshold among all possible thresholds that are out there, all possible splits that are out there. What's what the best one is? So we can first say that we're going to keep the best gain value here and I'll set it to minus one at first and then we'll also keep the split index and the split threshold. These are the things that we're going to return to none at first. And we're basically going to traverse all possible options here. So for all features in feature indices, we first need to get the column, this column, and that's going to be X and the index. So now I have only this column in my X column variable. I need to get all the possible thresholds here. So I can say MP, unique, again, give me all possible things that I can split with here. And then I'm going to traverse again for all the thresholds in thresholds. Maybe I'll call a THR to make it easier. So here I need to calculate the information gain and once I calculate the information gain, you can say gain, for example, we're going to calculate this with a helper function again. And then I can say if gain is better than the best gain that we acquired so far, the best gain is going to be gain and the split index is going to be this one, the index that we're working on right now and the split threshold is going to be the threshold. And once we're done with this, we can return the split index and the threshold. But I think I switched them here. Yeah, okay. So I'll just make sure that we're assigning it to the correct one. So at first it's the feature and then the threshold and it is correct here. All right. Now what we need to do is to calculate the information gain. So for that, I'm going to create another helper function here. So let's define it. Information gain. It's going to be self. What I need to pass it is the y and the x column and the threshold we're working with. That was threshold THR. All right. Let's check out how we calculated how we're supposed to calculate the information gain. So it is the entropy of a parent. So it looks like we're going to need yet another helper function here. And then the weighted average minus the weighted average of the entropy of the children. So first I need to get the parent entropy. Then I need to create children. Then calculate the weighted entropy of children. And then finally calculate the information gain using all of these things. But of course, we've weighted average of entropy of children. So the parent entropy is going to be entropy. Let's say self entropy. And I'm going to pass it y. Well, yeah, now that we're here, why don't we just define it actually? Entropy. It's going to get the path itself and y. Let's see how we are supposed to calculate entropy. So it is the summation of px times log 2 of px and px is, p of x is a number of times x has occurred divided by the number of values in here. So I'm going to use a numpy trick to do this. And I'll show you how I did that in a second. First, I'm going to count the y's. So what bing count does, kind of like a histogram, I'll show you over here. So let's say I have an array y and the values are 1, 2, 3, 1, 2. If I create a bing count for my y, what I'm going to get is 0, 2, 2, 1. So what does that mean? It means 0 has occurred 0 times. 1 has occurred 2 times 2 has occurred 2 times and 3 has occurred 1 time. Basically, I have like a nice little histogram here. So the next thing that I'm going to do is to divide this histogram by the number of values that I have. And why am I doing this? This is basically the p of x if we see here. It's basically a number of occurrences divided by the number of total values. So that's what I'm doing here, but I'm doing it for all of them basically at once. And the last thing we need to do for all the p's that we've created for p in p s. If I'll just write in one line, if this p is bigger than 0, we calculate this part basically. So p times log 2 of p. And nearly that's it. Of course, I also need to sum these up. And finally, I think we also need a minus sign here. And return this and that will be our entropy. All right, now with the this two, now we have the parent entropy. Let's create the children and calculate their entropy's too. So for that, I'm going to have to create yet another helper function here. Call us just split for now. And split will get the column. And that's where I show that we're getting here. Let's create it. Let's put it. The x column and the splitting threshold. And in here, I need to find which indices will go to the left and which will go to the right. So for that, I'm going to use numpy argweir. I'll show you how that one works. So let's say this is my x column. This is the values that I have in there. If I say x argweir, let's say x smaller than 3. Oops. And numpy argweir. Then it's going to tell me the 0, the first and the second one are smaller than 3. But of course, I don't want it to have it in a list of lists way. I want to have it in just one list. So that's why I will say flatten. So that's what we're going to do here. I will say, give me all the ones where the value of the x column is smaller than the split threshold. Smaller and equal to the split threshold. And then flatten it. So it's just an array. Same here. But except this time we're going to say bigger. Let's separate these a bit. So it's easier to read. And then we will return left indices and the right indices. And that's all actually. Now we have the left and the right indices here. After this, we should just check if one of these is empty. So if the left indices is empty or the right indices is empty, then we'll just return that the information gained from this split is zero. Okay, now it's time to calculate the vaded entropy of the children. So the first thing that I want to get is the length of the y. How many samples we have in the y and in the left and in the right. Left and number of samples in the right. That is length of the left indices and the length of the right indices. And need to add the ss here. And then we'll calculate their entropy. Entropy of the left and entropy of the right. We'll just going to pass it to the entropy helper function. The left indices. And the right indices. But of course we need to get the actual values from it. So not just the indices, but we need to find the avi values that belong to these indices. And finally, we calculate the calculated child entropy. And let's see how we did that. So it is basically the weighted average. So weighted average in this case means how many samples are in one of them and how many samples are in the other one. So I need to find out a number of samples in the left one divided by a total number of samples times left entropy plus number of samples in the right one divided by total number of samples times the right entropy. And that gives me the entropy of the children. We have the entropy of the parent. So the only thing that we need to do is to find the information gain and the information, not grain, gain. And that is parent entropy minus child entropy. And then we return information gain. There you go. We also finished the information gain function. So let's go back where did we leave it? All right. So what happens is when we call fit we check that the number of features is not more than the actual number of features. We call grow tree, grow tree checks, the best feature here. I first check sort of something criteria and then calls best split to find the best split. And the best split calls information gain. Information gain calls entropy and split. And split is run. It tells it what the information, what the left and the right trees are. I'm just checking that I don't have any typos at the same time. We calculate the entropy of the children. Like this. Let's see. And the information gain is calculated. It's passed back to best split. Best split is calculated here. And then it goes back to grow the tree. All right. So the last thing that we need to do is to create the child nodes then. And we already did part of it actually. So we call left indices and right indices. And guess what we're going to call for that? We're going to call split because split is a thing that calculates what the left and right indices should be. And the split requires x column and the split threshold. We know what the split threshold is. It is the best threshold. And the x column in this case is the column that belongs to the best threshold. So the best feature here. This will give me left and right indices. And now I have to call the grow tree using these left and right indices values. So the left would be grow tree. Makes left indices. What else do we need? We need the y. Y left indices. And finally, we need to increase the depth by one. The right will be more or less the same self. Grow tree. The right indices. And y right indices. And again, the depth will be plus one. And what we want to return here is a new node that we created with the information of what we divided it with, the best feature we divided it with, the threshold that we've divided it with. What else do we have here? And left and right. You also need to pass it left. And right. All right, so it looks like the part where we do the training actually ended. The next thing that we want to do is to write the predict function. Let's find that here. Right. So let's pass it to self. And an x test that we're going to get. So what I want to do is for every x and the set that is being passed to me, I want to traverse the tree to find the result. So one last helper function here. And that will return to me a list of results. And then I will parse it into a non-py array and return it. All right, let's start our last helper function, traverse tree. So the traversing of the tree will also happen in a recursive way. So I need to pass it to x. But I also need to pass it a node that we're starting from. So it first is going to be self root. We're going to start from the root. First, I want to check if this node is leaf node. What was the function there is leaf node. If that is the case, then we return the value of the node. So if you remember, when we're creating the left and right nodes here, we are passing them a certain feature, which feature that we divided it with. So we just need to find the feature that it's divided with. If the value is smaller or equal to the threshold, then we pass the left side of the tree to be traversed next. Or we pass the right side of the tree to be traversed next. So we return, call traverse tree again. We pass it to x again and now the starting node will be node left. Otherwise, though, if that is not the case, then we pass it the right. And yeah, that's all that we need to do to traverse the tree. We check if it's a leaf node. If that's the case, we pass the value. So basically, everything will end up here. All of the paths that we follow are going to end up in the leaf node anyways. But then we're going to get values and we're going to get one value for each number in our test set. And then we're going to turn it into an NMPI array and we're going to return it. Awesome. So it looks like we did everything. Let's run it and see if we made any mistakes or what the accuracy is going to be like. So for that, I'm importing the breast cancer data set again from CycatLearn. Need to import the decision tree and then we'll create a decision tree. And the first run, I'll just keep the default values that we already decided on. And then we're going to fit the decision tree with X train and Y train. And then we're going to get predictions using the predict method. And I'm going to pass it the X test for that. To see the results and how good the results are, of course, we need to have a metric. We'll call accuracy for now. I'll call predictions or maybe let's call Y-Pred. Y-Tests and Y-Pred. And that's basically the number of times Y-Tests equals to Y-Pred divided by length of the Y-Test. And we can just return this. And yeah, let's calculate the accuracy. I'll call it ACC. And it's going to be again, predictions and Y-Test. And let's print it afterwards. All right, I'm excited. Let's see. Oh, there. Line 51. Did a little comma here. Accidentally. Let's try again. All right, it took me a second, but I found what I did wrong here. And the definition, the initialization of the node, instead of self-value, equals to none, it should have been value and not none. And while I was figuring it out, I found another problem. In the traverse tree function, I was returning nodes value. But then I was calling the value. I'm not supposed to call it. I'm just supposed to return the value of this node. All right, so let's run this once again and see what we get. All right, so we get a accuracy of 0.91, which is pretty good, for something that we wrote in a couple of minutes. And let's try passing it some other arguments, for example, mac step 10. Okay, we get a bit of a better accuracy here, but yeah, you can play around with this and see what you get better on this. Brescancere data sets. I hope everything was clear. I know this was a bit of a longer one, because there's a bit more involved to create decision trees. If you have any questions, don't forget to leave them as a comment. We'll be happy to help you there. And if you want this code, you can find it in our GitHub repository, as always, the link is in the description. But I'm looking forward to seeing you in the next lesson.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 200)\n",
    "text  = text_splitter.split_text(transcription[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "db = Chroma.from_texts(text , embeddings_model)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert who knows about everything , \n",
    "Answer the user queries based on the given context which is based on a youtube video and\n",
    "also enhance the answers before giving it.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{input}\n",
    "</questions>\n",
    "\n",
    "\"\"\", input_variables=[\"context\" , \"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]  = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_chain = create_stuff_documents_chain(llm = llm , prompt=prompt)\n",
    "retrieval_chain  = create_retrieval_chain(retriever=retriever , combine_docs_chain=combine_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieval_chain.invoke({\"input\":\"Summarize the video's content in 100 words\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This video explains the process of building a decision tree. It uses NumPy's `bincount` function to create a histogram of value occurrences, which helps calculate the probability of each value. The video details how the tree is built during training by calculating the information gain from each potential split (feature and value) and dividing the dataset accordingly. This process repeats for newly created nodes until a stopping criterion is met. The video also mentions that the code used is available in the description.\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This video explains the process of building a decision tree. It uses NumPy's `bincount` function to create a histogram of value occurrences, which helps calculate the probability of each value. The video details how the tree is built during training by calculating the information gain from each potential split (feature and value) and dividing the dataset accordingly. This process repeats for newly created nodes until a stopping criterion is met. The video also mentions that the code used is available in the description.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
