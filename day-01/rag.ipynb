{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_loader = PyPDFLoader(\"svm.pdf\")\n",
    "documents_raw = pdf_loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(documents=documents_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11340\\3458987522.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma()\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "db = Chroma()\n",
    "embeddings_vectorstore = db.from_documents(documents,embedding_model)\n",
    "retriever = embeddings_vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import ollama\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ollama.Ollama(model=\"mistral:latest\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are an assistant for explaining things in a clear and concise manner,\\\n",
    "        Use the context to give a detailed answer based on the user query, \\\n",
    "            <context> {context} </context> \\\n",
    "            <query> {input} </query>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever=retriever,combine_docs_chain=combine_docs_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is svm explain it briefly in about 200 words',\n",
       " 'context': [Document(metadata={'page': 2, 'source': 'svm.pdf'}, page_content='Support Vector Machines formulation \\nSupport Vector m achines realize th e ideas o utlined above. To  see wh y, we n eed specify two  things: the \\nhypothesis sp aces u sed by SVM, an d the loss fun ctions used. The folklore v iew of SVM is t hat th ey find  an \\n\"optimal\" hyperpl ane as t he solution to the learni ng problem. The si mplest formulation of SVM  is the linear one, \\nwhere the hyperpla ne lies on the space of the input dat a x. In this case the hypot hesis space is a subset of all \\nhyperplanes of  the form:  \\nf(x) = w⋅x +b. \\nIn their m ost general form ulation, SVM fi nd a hyperplane i n a space diffe rent from  that of the input dat a x. It is a  \\nhype rplane in a feature space  induced by a kernel K (the kernel define s a dot product in that space (Wahba , 1990)). \\nThrough the kernel K the hy pothesis space  is defined as a set of \"hyperplanes \" in the feature s pace induce d by K.'),\n",
       "  Document(metadata={'page': 5, 'source': 'svm.pdf'}, page_content='standa rd SVM. On the other hand, they mention than using non-liner SVM s they can further improve performance. \\nCONCL USIONS \\nThe rep ort presented an overview o f the theory o f SVM in  parallel with  a summary o f the papers presented in the \\nACAI 99 workshop on \"Support Vector   Mach ines: theor y and applications\". Some of the important conclusions of \\nthis rep ort as well as o f the works hop are  sum marized bel ow:'),\n",
       "  Document(metadata={'page': 1, 'source': 'svm.pdf'}, page_content='(Vapnik, 1998) (Cortes a nd Vapni k, 1995), a nd have been success fully applied to a number  of applications , rangin g \\nfrom  time seri es pre diction (Fernandez, 1999), to face rec ognition (Tefas et al ., 1999), to biol ogical data proc essing \\nfor m edical diagnosis (Ve ropoulos et al., 1999). Their t heoretical founda tions a nd their expe rimental success  \\nencourage  further resea rch on their c haracter istics, as well as th eir further use.  \\nIn this repo rt we presen t a brief in troduction to the theory and implementation of SVM, an d we discuss the five \\npapers presented during the wo rkshop. The report is or ganized as follows: section 2 presents  the theoretical \\nfoundations of SVM . A brief overview of statistical learning theory also usi ng the discussi on in (Vay atis and \\nAzenc ott, 1999) is given. The mathematical formulation of SVM  is prese nted, and theory for the implementation of'),\n",
       "  Document(metadata={'page': 1, 'source': 'svm.pdf'}, page_content='Azenc ott, 1999) is given. The mathematical formulation of SVM  is prese nted, and theory for the implementation of \\nSVM, as in  (Trafalis, 1 999), is b riefly d iscussed. Section  3 summ arizes the experim ental work of (Veropoulos et al. , \\n1999), (Fernandez, 1999) and ( Tefas et al., 1999) and the variations of the standard SVM proposed in these pape rs. \\nFinally section 4 present s some concl usions and suggestions for future research . \\nA BRIEF OVERVIEW  OF THE SVM THEORY \\nSupport Vect or Machines have been  developed in the framework of Statistical Learni ng Theory - see f or \\nexample (Vapnik, 1998). We first briefly discuss s ome basic ideas of the t heory. \\nStatistical Learning Theory: a primer \\nIn statistical learning theory (SLT) t he problem of su pervised learni ng is formulated as f ollows. We are given a \\nset o f l train ing data {(x1,y1)...(xl,yl)} in Rn × R sam pled according to unknown  probability d istribution P(x,y), and a')],\n",
       " 'answer': ' Support Vector Machines (SVM) are a type of supervised machine learning algorithm used for classification and regression analysis. The primary goal of SVM is to find an optimal hyperplane that separates the data points of different classes in a way that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class (support vectors).\\n\\nIn simpler terms, SVM attempts to find the best line or plane that can accurately separate the given dataset into distinct groups. The hyperplane identified by SVM is not necessarily linear; it may be a curve in high-dimensional spaces. To handle nonlinear data, SVM uses a kernel trick to map the input data to a higher dimension where the data can be linearly separable.\\n\\nThe SVM algorithm involves solving a quadratic optimization problem, which finds the coefficients of the hyperplane (w and b) that maximize the margin while minimizing an error function. The error function measures the misclassification rate of the support vectors. By optimizing this function, SVM can find the hyperplane that provides the best balance between generalization performance (avoiding overfitting) and fitting the training data accurately.\\n\\nSVM has been successfully applied to various applications, such as time series prediction, face recognition, medical diagnosis, and more. Its theoretical foundations and experimental success encourage further research into its characteristics and potential uses. The report mentions that there are variations of standard SVM proposed in several papers, which can potentially improve the performance of SVM for specific tasks.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\":\"What is svm explain it briefly in about 200 words\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a type of supervised machine learning algorithm used for classification and regression analysis. The primary goal of SVM is to find an optimal hyperplane that separates the data points of different classes in a way that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class (support vectors).\\n\\nIn simpler terms, SVM attempts to find the best line or plane that can accurately separate the given dataset into distinct groups. The hyperplane identified by SVM is not necessarily linear; it may be a curve in high-dimensional spaces. To handle nonlinear data, SVM uses a kernel trick to map the input data to a higher dimension where the data can be linearly separable.\\n\\nThe SVM algorithm involves solving a quadratic optimization problem, which finds the coefficients of the hyperplane (w and b) that maximize the margin while minimizing an error function. The error function measures the misclassification rate of the support vectors. By optimizing this function, SVM can find the hyperplane that provides the best balance between generalization performance (avoiding overfitting) and fitting the training data accurately.\\n\\nSVM has been successfully applied to various applications, such as time series prediction, face recognition, medical diagnosis, and more. Its theoretical foundations and experimental success encourage further research into its characteristics and potential uses. The report mentions that there are variations of standard SVM proposed in several papers, which can potentially improve the performance of SVM for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
